{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "csv_files = glob.glob('hashtags/*.{}'.format('csv'))\n",
    "\n",
    "df_concat = pd.concat([pd.read_csv(f,usecols=(1,2)) for f in csv_files], ignore_index=True)\n",
    "\n",
    "df_concat.to_csv('hashtags_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3330 entries, 0 to 3995\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   avatar    3330 non-null   object\n",
      " 1   hashtags  3330 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 78.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "df = df_concat\n",
    "\n",
    "# indexEmpty = df[ (df['hashtags'])].index\n",
    "\n",
    "# df.drop(indexEmpty, inplace=True)\n",
    "df = df[df.astype(str)['hashtags'] != '[]']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from wordsegment import load, segment\n",
    "load()\n",
    "\n",
    "def decomposed_hashtags(hashtags):\n",
    "    return list(set(segment(hashtags)))\n",
    "    # return set()\n",
    "\n",
    "# # df_decomposed = df.head(100)\n",
    "# #\n",
    "# # df_decomposed['words'] = df_decomposed.apply(lambda row: decomposed_hashtags(row['hashtags']),axis=1)\n",
    "# #\n",
    "# # df_decomposed\n",
    "# # df_100.to_csv('decomposed_hashtags_1000.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyspark                                 # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import urllib\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark_df = spark.createDataFrame(df.head(100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "               avatar                                           hashtags\n0          sinemunsal        ['#tbt', '#pink', '#AydÄ±nlÄ±kevler,', '#tb']\n1           ellekorea  ['#ELLECoverstar', '#ì œì‘ë¹„ì§€ì›', '#ê³µìœ ', '#GONGYOO'...\n2            ashanegi  ['#mentalbehene', '#instareels', '#reelitfeeli...\n3      portadosfundos  ['#BBB21.', '#cozinhacompimenta', '#bbb', '#bb...\n4           tobibakre  ['#WellnessWithJumia', '#Camon16', '#CBeyond',...\n..                ...                                                ...\n95        baranikster  ['#×©×™×¨×œ×™×•××¨××©×•×Ÿ', '#×˜×•× ×”', '#ğŸŸ', '#×œ×¤×™×”×”× ×—×™×•×ª'...\n96       movierapture  ['#Galaxyofentertainment', '#cinephile', '#mov...\n97  photo._.grammetry  ['#noodles', '#reelsinstagram', '#ketofy', '#r...\n98     hiddengemindia  ['#foodreels', '#cakeofinstagram', '#rasmalaic...\n99   thegianninatwins  ['#arbonne', '#bonbabe', '#coffeelover', '#pla...\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>avatar</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sinemunsal</td>\n      <td>['#tbt', '#pink', '#AydÄ±nlÄ±kevler,', '#tb']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ellekorea</td>\n      <td>['#ELLECoverstar', '#ì œì‘ë¹„ì§€ì›', '#ê³µìœ ', '#GONGYOO'...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ashanegi</td>\n      <td>['#mentalbehene', '#instareels', '#reelitfeeli...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>portadosfundos</td>\n      <td>['#BBB21.', '#cozinhacompimenta', '#bbb', '#bb...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tobibakre</td>\n      <td>['#WellnessWithJumia', '#Camon16', '#CBeyond',...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>baranikster</td>\n      <td>['#×©×™×¨×œ×™×•××¨××©×•×Ÿ', '#×˜×•× ×”', '#ğŸŸ', '#×œ×¤×™×”×”× ×—×™×•×ª'...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>movierapture</td>\n      <td>['#Galaxyofentertainment', '#cinephile', '#mov...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>photo._.grammetry</td>\n      <td>['#noodles', '#reelsinstagram', '#ketofy', '#r...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>hiddengemindia</td>\n      <td>['#foodreels', '#cakeofinstagram', '#rasmalaic...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>thegianninatwins</td>\n      <td>['#arbonne', '#bonbabe', '#coffeelover', '#pla...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/14 21:01:52 WARN StopWordsRemover: Default locale set was [en_RO]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (1 + 7) / 8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/crangaantonio/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [56]\u001B[0m, in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m remover \u001B[38;5;241m=\u001B[39m StopWordsRemover(inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwords_filtered\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwords_without_stopwords\u001B[39m\u001B[38;5;124m\"\u001B[39m, locale\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men_US\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m wordsData \u001B[38;5;241m=\u001B[39m remover\u001B[38;5;241m.\u001B[39mtransform(wordsData)\n\u001B[0;32m---> 16\u001B[0m \u001B[43mwordsData\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoPandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m cv \u001B[38;5;241m=\u001B[39m CountVectorizer(inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwords_without_stopwords\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTF\u001B[39m\u001B[38;5;124m\"\u001B[39m, vocabSize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m) \u001B[38;5;66;03m# keep the most 1000 common words\u001B[39;00m\n\u001B[1;32m     19\u001B[0m cvModel \u001B[38;5;241m=\u001B[39m cv\u001B[38;5;241m.\u001B[39mfit(wordsData)\n",
      "File \u001B[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/pandas/conversion.py:205\u001B[0m, in \u001B[0;36mPandasConversionMixin.toPandas\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;66;03m# Below is toPandas without Arrow optimization.\u001B[39;00m\n\u001B[0;32m--> 205\u001B[0m pdf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_records(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[1;32m    206\u001B[0m column_counter \u001B[38;5;241m=\u001B[39m Counter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns)\n\u001B[1;32m    208\u001B[0m corrected_dtypes: List[Optional[Type]] \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema)\n",
      "File \u001B[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py:817\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    807\u001B[0m \u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[1;32m    808\u001B[0m \n\u001B[1;32m    809\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001B[39;00m\n\u001B[1;32m    815\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[0;32m--> 817\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1313\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1322\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import IDF, Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "\n",
    "wordsData = spark_df\n",
    "\n",
    "decompose_hashtags_udf = udf(lambda row: decomposed_hashtags(row), StringType())\n",
    "\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "\n",
    "wordsData = wordsData.withColumn('words',decompose_hashtags_udf(col('hashtags')))\n",
    "wordsData =wordsData.withColumn('words_filtered',filter_length_udf(col('words')))\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words_filtered\", outputCol = \"words_without_stopwords\", locale=\"en_US\")\n",
    "wordsData = remover.transform(wordsData)\n",
    "wordsData.toPandas()\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"words_without_stopwords\", outputCol=\"TF\", vocabSize=1000) # keep the most 1000 common words\n",
    "cvModel = cv.fit(wordsData)\n",
    "wordsData = cvModel.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"TF\", outputCol=\"features\")\n",
    "idfModel = idf.fit(wordsData)\n",
    "wordsData = idfModel.transform(wordsData)\n",
    "\n",
    "wordsData.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
